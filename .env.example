# Ollama Configuration (Local LLM)
# For Docker: use http://host.docker.internal:11434
# For local development: use http://localhost:11434
OLLAMA_HOST=http://localhost:11434
LLM_MODEL=llama3.1:8b
EMBEDDING_MODEL=nomic-embed-text

# LlamaIndex Configuration
# Set to 'true' to use LlamaIndex RAG pipeline, 'false' for legacy implementation
USE_LLAMAINDEX=true
# Similarity threshold for vector search (0.0-1.0)
SIMILARITY_THRESHOLD=0.7
# Number of similar chunks to retrieve for RAG
SIMILARITY_TOP_K=5
# Response synthesis mode: compact, tree_summarize, simple_summarize
RESPONSE_MODE=compact

# Neo4j Configuration
# Cloud option: Get a free AuraDB instance from https://neo4j.com/cloud/aura-free/
# Local option: Use docker-compose.neo4j.yml for local Neo4j instance
NEO4J_URI=neo4j+s://your-aura-db-uri.databases.neo4j.io
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_password
NEO4J_DATABASE=neo4j

# Qdrant Configuration (Vector Database)
# For Docker: use service name 'qdrant'
# For local development: use 'localhost'
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=code_chunks

# Redis Configuration (Caching)
# For Docker: use service name 'redis'
# For local development: use 'localhost'
REDIS_HOST=localhost
REDIS_PORT=6379
# Cache TTL in seconds
EMBEDDING_CACHE_TTL=86400  # 24 hours
QUERY_CACHE_TTL=3600       # 1 hour

# Storage and Logging
STORAGE_PATH=./data
LOG_LEVEL=INFO
LOG_PATH=./logs

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
# Set to 'true' for development hot-reload
API_RELOAD=false

# Docker-specific environment variables
# These are automatically set by docker-compose.yml
# OLLAMA_HOST=http://host.docker.internal:11434
# QDRANT_HOST=qdrant
# REDIS_HOST=redis
# STORAGE_PATH=/app/data

# GitHub OAuth Configuration
# Register an app at https://github.com/settings/developers
GITHUB_CLIENT_ID=
GITHUB_CLIENT_SECRET=
GITHUB_REDIRECT_URI=http://localhost:8080/connectors/github/callback
